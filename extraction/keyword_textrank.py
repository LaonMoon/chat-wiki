# -*- coding: utf-8 -*-
"""Keyword_textrank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rweltg0GltqaRmH71rY0j_SALTy6y7xS
"""

!pip install --upgrade gensim==3.8

from google.colab import drive
drive.mount('/content/drive')

import re
import pandas as pd


def katalk_msg_parse(file_path):
    my_katalk_data = list()
    katalk_msg_pattern = "[0-9]{4}[년.] [0-9]{1,2}[월.] [0-9]{1,2}[일.] 오\S [0-9]{1,2}:[0-9]{1,2},.*:"
    date_info = "[0-9]{4}년 [0-9]{1,2}월 [0-9]{1,2}일 오\S [0-9]{1,2}:[0-9]{1,2}"
    #in_out_info = "[0-9]{4}[년.] [0-9]{1,2}[월.] [0-9]{1,2}[일.] 오\S [0-9]{1,2}:[0-9]{1,2}, [이가].*"

    for line in open(file_path):
        if re.match(date_info, line): #or re.match(in_out_info, line):
            continue
        elif line == '\n':
            continue
        elif re.match(katalk_msg_pattern, line):
            line = line.split(",")
            date_time = line[0]
            user_text = line[1].split(" : ", maxsplit=1)
            user_name = user_text[0].strip()
            text = user_text[1].strip()
            my_katalk_data.append({'date_time': date_time,
                                   'user_name': user_name,
                                   'text': text
                                   })

        else:
            if len(my_katalk_data) > 0:
                my_katalk_data[-1]['text'] += "\n"+line.strip()

    my_katalk_df = pd.DataFrame(my_katalk_data)

    return my_katalk_df

file = pd.read_csv('/content/drive/MyDrive/TextWiki/wiki_text.txt', delimiter = '\t', encoding = 'utf-8')
file

content_all = []
for i in file['2023년 3월 10일 오전 12:51']:
  content = i.split(',',2)
  try:
    content2 = content[1].split(':',2)
    content_all.append(content[0] + ',' + content2[0] + ',' + content2[1])
  except:
      continue
content_all

date = []
name = []
content = []
for i in content_all:
  j = i.split(',',3)
  date.append(j[0])
  name.append(j[1])
  content.append(j[2])

content_df = pd.DataFrame(data = date, columns = ['data'])
content_df['name'] = name
content_df['content'] = content

content_series = content_df['content']
content_series

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# apt-get update
# apt-get install g++ openjdk-8-jdk python-dev python3-dev
# pip3 install JPype1
# pip3 install konlpy

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from konlpy.tag import Okt

content_df2 = content_df.copy()
content_series2 = content_df2['content']
content_series2

import re

def message_cleaning(docs):
  
  docs = [str(doc) for doc in docs]

  clean1 = re.compile("<사진|<동영상")
  docs = [clean1.sub("",doc) for doc in docs]
  
  clean2 = re.compile("[ㄱ - ㅎ]*[ㅏ - ㅢ]*")
  docs = [clean2.sub("",doc) for doc in docs]
  
  clean3 = re.compile("(http|ftp|https)://(?:[-\w.]|(?:%[\da-fA-F]{2}))+")
  docs = [clean3.sub("",doc) for doc in docs]
  
  return docs

#불용어 제거 

def define_stopwords(path):
  SW = set()
  with open(path, encoding = 'utf-8') as f:
    for word in f:
      SW.add(word)
  return SW

SW = define_stopwords('/content/drive/MyDrive/TextWiki/stopwords.txt')
cleaning = message_cleaning(content_series2)
print(len(cleaning))
print(cleaning[:10])

cleaning2 = []
for i in cleaning:
  cleaning2.append(i.strip())

cleaned_text = pd.Series(cleaning2)
content_df2['content'] = cleaned_text

cleaned_data = content_df2[content_df2['content'] != ""]

cleaned_data.info() 
cleaned_data.head(10)

total_tokens = [token for msg in content_series for token in msg.split()]
print(len(total_tokens))

text = nltk.Text(total_tokens, name = 'NMSC')
print(len(set(text.tokens)))
print(text.vocab().most_common(50))

from gensim.summarization.summarizer import summarize
from gensim.summarization import keywords
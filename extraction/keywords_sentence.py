# -*- coding: utf-8 -*-
"""keywords_sentence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqCVxiA9DZZe6XMDbd-XyPBi7tquMO3f
"""

!pip install --upgrade gensim==3.8
!pip3 install JPype1
!pip3 install konlpy

from google.colab import drive
drive.mount('/content/drive')

import re
import pandas as pd

file = pd.read_csv('/content/drive/MyDrive/TextWiki/wiki_text.txt', delimiter = '\t', encoding = 'utf-8')
file

#대화만 추출 
content_all = []
for i in file['2023년 3월 10일 오전 12:51']:
  content = i.split(',',2)
  try:
    content2 = content[1].split(':',2)
    content_all.append(content[0] + ',' + content2[0] + ',' + content2[1])
  except:
      continue
content_all

# date, name, content dataframe으로 나타내기 
date = []
name = []
content = []
for i in content_all:
  j = i.split(',',3)
  date.append(j[0])
  name.append(j[1])
  content.append(j[2])

content_df = pd.DataFrame(data = date, columns = ['data'])
content_df['name'] = name
content_df['content'] = content

content_series = content_df['content']
content_series

from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()
kkma = Kkma()

final_text = []

for line in content_df['content']:
  print(okt.nouns(line))
  final_text.append(okt.nouns(line))

for i in final_text:
  final_str = ' '.join(i)
  print(final_str)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from konlpy.tag import Okt

from nltk.tokenize import WordPunctTokenizer

from gensim.summarization.summarizer import summarize
from gensim.summarization import keywords

#복사
content_df2 = content_df.copy()
content_series2 = content_df2['content']
content_series2

#data cleaning
def message_cleaning(docs):
  
  #docs = [str(doc) for doc in docs]

  clean1 = re.compile("<사진|<동영상")
  docs = [clean1.sub("",doc) for doc in docs]
  
  clean2 = re.compile("[ㄱ-ㅎ]*[ㅏ-ㅢ]*")
  docs = [clean2.sub("",doc) for doc in docs]
  
  clean3 = re.compile("(http|ftp|https)://(?:[-\w.]|(?:%[\da-fA-F]{2}))+")
  docs = [clean3.sub("",doc) for doc in docs]

  clean4 = re.compile("삭제된 메세지입니다.")
  docs = [clean4.sub("",doc) for doc in docs]
  
  return docs

#불용어 제거 

def define_stopwords(path):
  SW = set()
  with open(path, encoding = 'utf-8') as f:
    for word in f:
      SW.add(word)
  return SW

SW = define_stopwords('/content/drive/MyDrive/TextWiki/stopwords.txt')
cleaning_sen = message_cleaning(content_series2)
print(len(cleaning_sen))
print(cleaning_sen[:20])

cleaned = []
for i in cleaning_sen:
  cleaned.append(i.strip())

cleaned_text = pd.Series(cleaned)
content_df2['content'] = cleaned_text

cleaned_data = content_df2[content_df2['content'] != ""]

for line in cleaning_sen:
  print(keywords(line, words=1).split('\n'))